

\section{Fundamentals}

\begin{figure}[t!]
\centering
\includegraphics[scale=0.5]{figs2/lstm3.png}
\caption{LSTM structure diagram.}
\label{fig:lstm}
\end{figure}
\subsection{Long Short Term Memory (LSTM)}

Recurrent neural networks (RNN) are a type of neural network designed for processing sequential data. These type of models are state-of-the-art in temporal modeling tasks\cite{ma2017ts}. In particular, LSTM are a special type of RNN that are capable of modeling both long and short term time dependencies. Their main improvement is an additional hidden state $C_t$ which acts as a memory cell that can be accessed, written and cleared by several self-parametrized, trainable gates. Specifically, the model uses an information gate $i_t$ to select which information is added to the cell; a forget gate $f_t$ to discard unuseful previous knowledge and an output gate $o_t$ to produce the final result. Architecture for this model is presented in Figure \ref{fig:lstm}.
\subsection{Convolutional Long Short Term Memory (ConvLSTM)}

A major drawback from the LSTM in handling spatial data is the usage of fully connected layers for its input-to-state and state-to-state transitions, which don't take spatial context into account. To overcome this problem, a ConvLSTM cell takes the original LSTM and replaces the fully connected layers from the forget, information and output gates with convolutions. Inputs $\textbf{x}_1,...\textbf{x}_t$, hidden states $\textbf{h}_1,...\textbf{h}_t$ and cell outputs $\textbf{C}_1,...\textbf{C}_t$ are 3D tensors (Multi-channel images) \cite{xingjian2015convolutional}.

\subsection{Fully Convolutional Network (FCN)}

Typical CNNs contain fully connected (F.C.) layers that don't consider the spatial information, producing non-spatial outputs. The Fully Convolutional Network (FCN) removes the final classifier layer from the CNN and converts all the fully connected layers to convolutions. This way, the final output is a classification map with semantic meaning. Typical segmentation architecture is composed of a downsampling path which extracts coarse semantic features, followed by an upsampling path responsible for recovering the input resolution for the final output. 

Dense Blocks (DB)  are  a  sequence  of  convolutional  layers  with  multiple bypassing connections between them. Up and down-sampling transitions  are  performed  with \textit{transpose convolution} and \textit{average pooling} operations respectively.

